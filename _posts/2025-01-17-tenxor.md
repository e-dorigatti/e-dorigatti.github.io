---
date: 2025-01-17 12:00:00 +0200
title: "Tensor computing from scratch"
layout: post
categories:
 - Deep Learning
 - Python
 - Math
 - Development
---

Squeeze, unsqueeze, view, reshape, flatten, swapaxes... Oh my! In deep learning applications, tensors can have extremely fluid shapes flowing from one operation to the next. Have you ever wondered how tensor computing libraries such as pytorch, numpy and tensorflow are able to efficiently keep track of your data, weights and activations? Then you've come to the right place, as we are going to implement our very own tensor library from scratch!

<!-- more -->

What are we trying to achieve? Essentially, we want to write a set of functions that work with with tensors, or multi-dimensional arrays, allowing users to perform basic mathematical operations such as summation, multiplication, and so on. 
This kind of operations are the basis for modern deep learning and artificial intelligence methods, as well as pretty much anything that involves large-scale numerical computations.
The [numpy][np] documentation has a nice introduction to the basic ideas of working with such type of data.

This post is available as a Jupyter notebook, so feel free to [download it][jn] and follow along.

 [np]: https://numpy.org/doc/stable/user/absolute_beginners.html#what-is-an-array
 [jn]: /attachments/tenxor.ipynb

## Data container

The fundamental idea underlying efficient storage of multi-dimensional arrays is to always keep the data into a simple one-dimensional array, or list.
Alongside, we also store the current shape of the tensor and a few more things which we will then use to correctly implement mathematical operations.

Let's start by implementing the most basic container:


```python
from functools import reduce
import random
import math
from typing import Any, Callable, List, Optional, Sequence, Tuple


class Tenxor:
    def __init__(self, values: List[float], shape: Optional[List[int]]):
        self.values = values

        if shape is not None:
            # the total number of elements must equal the product
            # of the size of each dimension
            shape_elements = reduce(lambda x, y: x * y, shape)
            if len(values) != shape_elements:
                raise ValueError(
                    f"shape {shape} incompatible with values of size {len(values)}"
                )
            self._shape = shape
        else:
            self._shape = [len(self.values)]

    def get_shape(self) -> List[int]:
        return self._shape

    def __eq__(self, other: "Tenxor") -> bool:
        """
        Two Tenxors are the same if they have the same data and same shape.
        """
        return self._shape == other._shape and all(
            x == y for x, y in zip(self.values, other.values)
        )

    def __repr__(self):
        vals = ", ".join(map(str, self.values[:5]))
        return (
            f'{self.__class__.__name__} of {len(self.values)} items'
            f' with shape {self._shape} and values [{vals} ... ]'
        )
```


```python
t = Tenxor([0] * 12, [6, 2])
t
```




    Tenxor of 12 items with shape [6, 2] and values [0, 0, 0, 0, 0 ... ]



## Pointwise transformations

The simplest kind of operations that we can implement at this point are pointwise transformations, i.e., transformations that involve each element separately.


```python
def ttransform(tenxor: Tenxor, op: Callable[[Any], Any]) -> Tenxor:
    """ Transforms all elements of the Tenxor with the given callable. """
    return Tenxor([op(x) for x in tenxor.values], tenxor.get_shape())
```


```python
def tmul_scalar(tenxor: Tenxor, scalar: float) -> Tenxor:
    """ Multiplication by a scalar """
    return ttransform(tenxor, lambda x: scalar * x)
```


```python
def tadd_scalar(tenxor: Tenxor, scalar: float) -> Tenxor:
    """ Addition with a scalar. """
    return ttransform(tenxor, lambda x: scalar + x)
```


```python
tadd_scalar(t, 5)
```




    Tenxor of 12 items with shape [6, 2]  and values [5, 5, 5, 5, 5 ... ]



## Changing shape

Since we keep the tensor shape separate from the actual data, changing shapes is a breeze: we just need to save the new shape somewhere!
The only restriction is that the new shape must imply the same number of elements as the old shape.
For convenience, it is often allowed to use the special size of `-1` to indicate that one dimension can take up as many elements as necessary to keep this invariance.
For example, suppose we want to reshape a tensor with shape `(6, 2)` into shape `(-1, 3, 1)`; the resulting shape would be `(4, 3, 1)`, since it implies the same number of elements.
If would not be possible to reshape `(7, 2)` into it, however, since you cannot arrange 14 elements into an integer number of rows of 3 elements each.


```python
def tview(old_shape: List[int], new_shape: List[int]) -> List[int]:
    """
    Return the new shape of the Tenxor after reshaping.
    """
    
    if any(s < -1 or s == 0 for s in new_shape):
        raise ValueError("Zero or negative shape other than -1 not allowed")

    # find if there is a dimension of size -1
    news = list(new_shape)
    fill_dimensions = [i for i, s in enumerate(new_shape) if s == -1]
    if len(fill_dimensions) > 1:
        raise ValueError("Only one dimension can be -1")
    elif fill_dimensions:
        # number of elements in the original tensor
        numel = reduce(lambda x, y: x * y, old_shape)
        
        # compute the total number of elements implied by the other dimensions
        other_ns = reduce(lambda x, y: x * y, (s for s in new_shape if s != -1))
        
        # this number must fit exactly into the total number of elements,
        # otherwise it is not possible to pack it into a single dimension
        if numel % other_ns != 0:
            raise ValueError(f"cannot reshape {old_shape} to {new_shape}")
        news[fill_dimensions[0]] = numel // other_ns

    return news


assert tview([6, 2], [-1, 3, 1]) == [4, 3, 1]
```

We now implement two very useful operations to add and remove dimensions of size `1`.
These operations are going to be extremely useful later once we implement "broadcasting", which allows us to work with Tenxors of different shapes.


```python
def tsqueeze(shape: List[int], dim: int) -> List[int]:
    """
    Return the new shape of the Tenxor after removing the given dimension of size 1.
    """
    
    if dim < 0:
        # support negative indexing from the end
        dim = len(shape) + dim
    
    # make sure that the dimension exists and has size 1
    if dim < 0 or dim > len(shape):
        raise ValueError("shape out of range")  
    elif shape[dim] != 1:
        raise ValueError(f"cannot unsqueeze dimension {dim} of size {shape[dim]}")

    # simply remove the given dimension
    new_shape = [s for i, s in enumerate(shape) if i != dim]

    return new_shape


assert tsqueeze([4, 3, 1], -1) == [4, 3]
```


```python
def tunsqueeze(shape: List[int], dim: int) -> List[int]:
    """
    Add a new dimension of size 1 in the given position.
    """
    
    if dim < 0:
        # support negative indexing from the end
        dim = len(shape) + dim + 1
        
    if dim < 0 or dim >= len(shape) + 1:
        # make sure that the dimension exists
        raise ValueError("shape out of range")

    # add a new dimension of size 1 in the right place
    new_shape = shape[:dim] + [1] + shape[dim:]
    
    return new_shape


assert tunsqueeze([4, 3], 0) == [1, 4, 3]
assert tunsqueeze([4, 3], -1) == [4, 3, 1]
```

## Element indexing

The key issue that we need to solve before moving on to more complex operations is to index the elements contained in a tensor.
Here, I do not mean indexing with brackets as in `t[0, 1, 2]`, but rather finding the position in the flattened item array corresponding to a certain multi-dimensional index.

This is where knowing the shape becomes crucial.
Suppose that we are working with a two-dimensional array of shape `(5, 3)`. It is natural to put the element `(0, 0)` at position `0` of the flat array.
Now, we are going to assume that the last dimension is the fastest that changes, so that the element `(0, 1)` goes into position `1`, and the element `(0, 2)` to position `2`.
The second-to-last dimension is the next one to change, so that `(1, 0)` goes to position `3`, `(1, 1)` to position `4`, and so on.
Generally, the item `(a, b)` of a `(5, 3)` array goes to position `3 * a + b`.
With more dimensions, we simply need to multiply the index with the shape of all previous dimensions combined.


```python
def position_to_index(pos: List[int], shape: List[int]) -> int:
    """
    Find the index in the flattened array corresponding to the given
    position in the multi-dimensional tensor. 
    """
    idx = 0
    acc = 1
    k = len(shape) - 1
    while k >= 0:
        if pos[k] >= shape[k]:
            raise RuntimeError(
                f"index {pos[k]} at dimension {k} out of bounds for size {shape[k]}"
            )
        idx += acc * pos[k]
        acc *= shape[k]
        k -= 1
    return idx


assert position_to_index(
    pos=[1, 2, 3], shape=[5, 6, 7]
) == 59  # = 3 + 2*7 + 1*7*6
```

We will also need a function for the inverse operation, i.e., mapping an index into the flat array back to an index into the multi-dimensional tensor.
In our running example, a flat index of `4` for a shape of `(5, 3)` should be mapped to position `(1, 2)` since `4 = 1 * 3 + 2`.

In the following function, I use the term "slice" to indicate the elements of the flat array spanned by each dimension of the tensor.
In other words, which and how many elements should be skipped each time the index of that dimension is incremented by one.
The very last slice always has size one, since incrementing the last index moves the index into the flat array by one.
The second-to-last slice has size equal to the last dimension: in our `(5, 3)` example, the element `(1, 2)` is separated from the element `(0, 2)` by exactly three elements.

In a tensor of shape `(5, 6, 7)`, the slices have size `(42, 7, 1)`, where 42 = 7 x 6.
This means that the elements `(i, j, k)` and `(i + 1, j, k)` are separated by 42 elements in the flattened array.
Suppose we want to find the multi-dimensional position of element 59 in the flattened array:
  1. Because 42 divides 59 one time with remainder of 17, the first dimension has index 1.
  2. The remaining 17 items are divided by 7 two times with remainder 3, so the second dimension has index 2.
  3. The remaining 3 items are divided by 1 3 times with remainder 0, so the third dimension has index 3. 


```python
def index_to_position(idx: int, shape: List[int]) -> int:
    """
    Finds the position in the multi-dimensional tensor corresponding to
    the element at the given index in the flattened array.
    """
    
    # we could make this function more efficient by pre-computing this
    # and storing it in the Tenxor
    slice_sizes = [1] * len(shape)
    for k in range(len(shape) - 1, 0, -1):
        slice_sizes[k - 1] *= shape[k] * slice_sizes[k]
    
    pos = [0] * len(shape)
    for k in range(len(shape)):
        pos[k] = idx // slice_sizes[k]
        idx = idx % slice_sizes[k]
            
        if pos[k] >= shape[k]:
            raise RuntimeError(
                f"index {pos[k]} at position {k} out of bounds"
                f" for slice size {slice_sizes[k]}"
            )

    return pos


assert index_to_position(59, [5, 6, 7]) == [1, 2, 3]
```

Finally, the last foundational method we need is one to find the position that is immediately "following" a given position.
With this method, we will be able to navigate a tensor visiting all its elements one after the other, thus building a fundation for complex binary operations between two tensors.  


```python
def next_position(pos: List[int], shape: List[int]) -> bool:
    """
    Find the multi-dimensional position immediately "after" the given position.
    Modifies `pos` in place, returns true if there are more elements to visit,
    or false if the computed position is out of bounds.
    """
    
    pos[-1] += 1
    k = len(pos) - 1
    while k > 0 and pos[k] == shape[k]:
        pos[k] = 0
        pos[k - 1] += 1
        k -= 1
    
    return pos[0] < shape[0]


p = [1, 2, 6]
next_position(p, [5, 6, 7])
assert p == [1, 3, 0]
```

Let's make sure that these methods can work together harmoniously by scanning all elements of a tenxor and ensuring that these methods give consistent results:


```python
pos = [0, 0, 0]    # initial position
shape = [2, 2, 2]  # shape of the tenxor

has_more = True
idx = 0
while has_more:
    print(f"Index {idx} corresponds to position {pos} for shape {shape}")
    assert pos == index_to_position(idx, shape)
    assert idx == position_to_index(pos, shape)
    
    has_more = next_position(pos, shape)
    idx += 1


print(f"First position out of bounds for shape {shape} is {pos}")
```

    Index 0 corresponds to position [0, 0, 0] for shape [2, 2, 2]
    Index 1 corresponds to position [0, 0, 1] for shape [2, 2, 2]
    Index 2 corresponds to position [0, 1, 0] for shape [2, 2, 2]
    Index 3 corresponds to position [0, 1, 1] for shape [2, 2, 2]
    Index 4 corresponds to position [1, 0, 0] for shape [2, 2, 2]
    Index 5 corresponds to position [1, 0, 1] for shape [2, 2, 2]
    Index 6 corresponds to position [1, 1, 0] for shape [2, 2, 2]
    Index 7 corresponds to position [1, 1, 1] for shape [2, 2, 2]
    First position out of bounds for shape [2, 2, 2] is [2, 0, 0]


This basic loop and its variations will form the basis by which we implement many operations later, so be sure to understand how it works!

## Tensor interface

We want to hide the fact that the data is stored in a simple list, so that users of our tensors can use it only via multi-dimensional indexing.
Therefore, let us create a basic class that defines the interface by which we will use the tensors, plus a few simple operations:


```python
class AbstractTenxor:
    
    # abstract interface
    def get_shape(self) -> List[int]:
        raise NotImplemented()
    
    def get_at_position(self, pos: List[int]) -> float:
        raise NotImplemented()
    
    def set_at_position(self, pos: List[int], val: float) -> None:
        raise NotImplemented()

    def next_position(self, pos: List[int]) -> bool:
        raise NotImplemented()

    def transform_values(self, op: Callable[[float], float]) -> 'AbstractTenxor':
        raise NotImplemented()

    # shape features
    def squeeze(self, dim: int) -> 'AbstractTenxor':
        new_shape = tsqueeze(self.get_shape(), dim)
        return self.view(new_shape)
    
    def unsqueeze(self, dim: int) -> 'AbstractTenxor':
        new_shape = tunsqueeze(self.get_shape(), dim)
        return self.view(new_shape)
       
    def view(self, shape: List[int]) -> 'AbstractTenxor':
        raise NotImplemented()
```

And provide an implementation of these methods for the basic container above:


```python
class Tenxor(AbstractTenxor):
    def __init__(self, values: List[float], shape: List[int]):
        self._values = values
        self._shape = shape
    
    def get_shape(self) -> List[int]:
        return self._shape
    
    def view(self, shape: List[int]) -> "Tenxor":
        return Tenxor(self._values, shape)
    
    def get_at_position(self, pos: List[int]) -> float:
        idx = position_to_index(pos, self._shape)
        return self._values[idx]
    
    def set_at_position(self, pos: List[int], val: float) -> None:
        idx = position_to_index(pos, self._shape)
        self._values[idx] = val
        
    def next_position(self, pos) -> bool:
        return next_position(pos, self._shape)
    
    def transform_values(self, op: Callable[[float], float]) -> 'Tenxor':
        return Tenxor([op(x) for x in self._values], self._shape)
    
    def __eq__(self, other: "Tenxor") -> bool:
        return self._shape == other._shape and all(
            x == y for x, y in zip(self._values, other._values)
        )
```

This will become useful later on in this post, as we are going to provide a few new tensor implementations with additional features.

## Reduction along one axis

Finally we can start with actual maths!
There are two fundamental operations that we should support: reduction along one axis, and pointwise operations between elements at the same position.
More complex operations can be implemented on top of these two as we are going to see later.

Let's start with reduction.
The goal here is to combine all elements that belong to a given dimension of the array, e.g. by summing them.
For example, since rows are the first dimension of a matrix, a reduction along the first dimension combines all elements of each column together, and the result is an one-dimensional array with one element for each column.
A reduction along the second axis works in the opposite way, combining the elements of each row.

You can think of the dimension that we reduce along as the dimension that "disappears" in the result.
For example, reducing an array of shape `(4, 3, 2)` along the second dimension results in an array of shape `(4, 2)`.
The element at position `(3, 1)` of the result is obtained by combining the elements at positions `(3, 0, 1)`, `(3, 1, 1)` and `(3, 2, 1)` of the input array.


```python
def tredux(
    tenxor: AbstractTenxor, dim: int, init: Any, redux: Callable[[Any, Any], Any]
) -> AbstractTenxor:
    """
    Combine all elements of the given dimension using the operation provided. 
    """
    
    if dim < 0:
        dim = len(tenxor.get_shape()) + dim
    if dim < 0 or dim > len(tenxor.get_shape()):
        raise ValueError("shape out of range")

    # Compute the shape of the result and initialize its elements.
    # At first we keep the dimension that we are reducing along, but with
    # only one element that contains the result along that slice.
    
    # initialize the result tensor
    res_shape = [s if i != dim else 1 for i, s in enumerate(tenxor.get_shape())]
    result = Tenxor([init] * reduce(lambda x, y: x * y, res_shape), res_shape)
    
    # initialize the counters
    res_pos = [0] * len(res_shape)
    res_has_more = True
    while res_has_more:
        # scan the input tensor along the given dimension and reduce all items there
        in_pos = res_pos[:]
        acc = init
        for i in range(tenxor.get_shape()[dim]):
            in_pos[dim] = i
            acc = redux(acc, tenxor.get_at_position(in_pos))
        result.set_at_position(res_pos, acc)

        # move to next result position
        res_has_more = result.next_position(res_pos)

    # finally remove the dimension that we reduced along
    return result.squeeze(dim)
```


```python
tx = Tenxor(range(24), (4, 3, 2))

# sum all elements of the first dimension
assert tredux(tx, 0, 0, lambda x, y: x + y) == Tenxor(
    [36, 40, 44, 48, 52, 56], [3, 2]
)

# sum all elements of the second dimension
assert tredux(tx, 1, 0, lambda x, y: x + y) == Tenxor(
    [6, 9, 24, 27, 42, 45, 60, 63], [4, 2]
)

# sum all elements of the third dimension
assert tredux(tx, 2, 0, lambda x, y: x + y) == Tenxor(
    [1, 5, 9, 13, 17, 21, 25, 29, 33, 37, 41, 45], [4, 3]
)
```

Note that in this function we visit each item of the result only once and reduce all input elements in one shot with the inner loop.
The other option would be to scan each item of the input, figure out where it should end up in the result, and perform the computation accordingly, thus eliminating the need for an inner loop.
The reason why we did it in this way is that it would be much easier to parallelize, for example on a GPU, since we could have in principle a thread for each item of the result, and all these threads could write independently of each other (having multiple threads writing to the same location is much slower since they would need to synchronize).

## Pointwise operations

Pointwise operations combine the elements of two input tensors that are located at the same position.
When the two tensors have the same shape everything is easy, but the real power of this function comes from combining tensors of different sizes.
How is it possible you ask? This is thanks to **broadcasting**, which is a fundamental concept to write more complex numerical routines.

Let's start with an example: the outer sum between a vector of size 4 and a vector of size 3, matrix of size `(4, 3)`.
The element at position `(i, j)` of this matrix comes from the sum of element `i` of the first vector and element `j` of the second vector.
But we want to compute this as the combination of two tensors of the same shape, so how can we get this result by adding together the elements at position `(i, j)` of two matrices of size `(4, 3)`?
Well, we could take the fist vector of size four, and create three copies of it along three columns of one matrix, then take the second vector of size three and create four copies of it along two rows of the second matrix, like this (image from the numpy documentation):

![broadcasting](https://numpy.org/doc/stable/_images/broadcasting_4.png)

This is essentially the core of broadcasting: we create copies of certain dimensions of the two input arrays, or "stretch" them, such that all of their dimensions match.
To make this efficient, we do not want to actually copy the data, we would rather work with the two inputs as they are.

Not all inputs can be broadcasted together, however; their shapes have to be compatible.
In general, two vectors can be broadcasted together as long as their shapes can be paired such that the sizes of each dimension are the identical, or one of them equals `1`.
For example, `(5, 1)` can be broadcasted with `(5, 3)`. 
Moreover, if one array has fewer dimensions, its shape can be extended to the left with additional dimensions of size `1`; for example, `(5, 1, 3)` can be broadcased with `(4, 3)`, since the latter shape can be extended to `(1, 4, 3)`, and the result would have shape `(5, 4, 3)`.
You can visualize this easily by writing the shapes and aligning each dimension of the inputs to the right:

```
(5, 1, 3)  # Shape of a
(   4, 3)  # Shape of b
(5, 4, 3)  # Shape of c = a + b
```

The actual computation performed is:

```
c = a + b
# c[i, j, k] = a[i, 0, k] + b[0, j, k]
```

The way to understand indexing is that an index for a dimension is not used for an input whenever its size is one, using 0 instead. 
Since the second dimension of `a` has size 1, then the second dimension of the result does *not* come from `a`, i.e., the index `j` is not used for `a`.
In the same way, since the first dimension of `b` is 1 after broadcasting, the first index of the result, that is `i`, is not used to index `c`.
This is exactly how we are going to implement broadcasting: ignore indices corresponding to dimensions of size 1.
Feel free to check [numpy's documentation on broadcasting][npbc] for more detailed explanation and examples.

 [npbc]: https://numpy.org/doc/stable/user/basics.broadcasting.html


```python
def broadcast_shapes(
    shape_a: List[int], shape_b: List[int]
) -> Tuple[List[int], List[int], List[int]]:
    """
    Broadcast the shapes of two tensors, returning the new shapes after broadcasting as well
    as the shape of the result.
    """
    
    # match the length of the shapes by extending
    # the shorter one with ones to the left
    an, bn = len(shape_a), len(shape_b)
    if an == bn:
        pass
    elif an > bn:
        shape_b = [1] * (an - bn) + shape_b
    elif an < bn:
        shape_a = [1] * (bn - an) + shape_a
    
    # check that shapes are compabible, and compute the shape of the result
    result_shape = []
    for k, (n, m) in enumerate(zip(shape_a, shape_b)):
        r = None
        if n == 1:
            # dimension k of the first tensor is broadcasted
            # use dimension k of the second tensor as
            # size for this dimension of the result
            r = m
        elif m == 1:
            # dimension k of the second tensor is broadcasted
            # use dimension k of the first tensor as
            # size for this dimension of the result
            r = n
        elif m != n:
            # input tensors have different shapes that are not broadcastable,
            # i.e., different than one
            raise ValueError(
                f'incompatible shapes {shape_a} and {shape_b} at dimension {k}'
            )
        else:
            r = m
        
        result_shape.append(r)
    
    return shape_a, shape_b, result_shape


assert broadcast_shapes([5, 1, 3], [7, 1, 4, 3]) == (
    [1, 5, 1, 3],  # dimension of size one added to the right of shape_a
    [7, 1, 4, 3],  # shape_b remains the same
    [7, 5, 4, 3],  # dimension of the result with the largest size for each axis
)
```

We can now use this function to align the shapes of the two tensors and compute the result shape before doing the point-wise operation:


```python
def tpoint(
    a: Tenxor, b: Tenxor, redux: Callable[[Any, Any], Any]
) -> Tenxor:
    """
    Perform a pointwise operation by combining the two elements of the inputs
    that are in the same position after broadcasting.
    """
    
    # broadcast tensors and compute shape result
    a_shape, b_shape, result_shape = broadcast_shapes(a.get_shape(), b.get_shape())
    a_broad = a.view(a_shape)
    b_broad = b.view(b_shape)
    
    # initialize the container data for the result tensor and the position cursor
    result = Tenxor([None] * reduce(lambda m, n: m * n, result_shape), result_shape)
    result_pos = [0] * len(result_shape)
    result_has_more = True
    
    # perform the operation
    while result_has_more:
        # find the positions in the input arrays:
        # use 0 in each dimension that was broadcasted
        pos_a = [i if n > 1 else 0 for i, n in zip(result_pos, a_broad.get_shape())]
        pos_b = [i if n > 1 else 0 for i, n in zip(result_pos, b_broad.get_shape())]
        
        # perform the operation, store the result and move on
        res = redux(a_broad.get_at_position(pos_a), b_broad.get_at_position(pos_b))
        result.set_at_position(result_pos, res)
        result_has_more = result.next_position(result_pos)

    return result


assert tpoint(
    Tenxor([1, 2, 3], (3, 1)),
    Tenxor([1, 2, 3], (1, 3)),
    lambda x, y: x * y  # element-wise product
) == Tenxor([1, 2, 3, 2, 4, 6, 3, 6, 9], [3, 3])
```

The operation above computed the outer product of $\vert 1, 2, 3 \vert$ as a column vector with its transpose, i.e. a row vector, giving a 3 by 3 matrix:

$$
\begin{vmatrix} 1 \\ 2 \\ 3 \end{vmatrix}
\times
\begin{vmatrix} 1 & 2 & 3 \end{vmatrix}
=\begin{vmatrix}
1 & 2 & 3 \\
4 & 6 & 8 \\
3 & 6 & 9
\end{vmatrix}
$$

If you are short on time, this is the bare essential to know on tensor computing from scratch!

But if you are thirsty for more, including the implementation of a graph neural network, transpositions, and slicing, keep reading!

## Enhancing user experience

Even with the minimal functions we wrote above, we can already perform some serious math!
But they are also quite cumbersome to use.
Therefore, before applying our tensor library to a real use-case, let's write a few simple utilities to make our life simpler.


```python
global_rng = random.Random(123)

def trandom(*shape: int, seed: Optional[int] = None) -> Tenxor:
    """
    Create a tensor with the given shape and elements sampled randomly in [-1, 1]
    """
    numel = reduce(lambda x, y: x * y, shape)
    
    rng = global_rng if seed is None else random.Random(seed)
    return Tenxor([2 * rng.random() - 1 for _ in range(numel)], shape=list(shape))
```


```python
def tadd(a: Tenxor, b: Tenxor) -> Tenxor:
    """ Pointwise addition of two tensors """
    return tpoint(a, b, lambda x, y: x + y)
```


```python
def tmul(a: Tenxor, b: Tenxor) -> Tenxor:
    """ Pointwise multiplication of two tensors """
    return tpoint(a, b, lambda x, y: x * y)
```


```python
def tdiv(a: Tenxor, b: Tenxor) -> Tenxor:
    """ Pointwise division of two tensors """
    return tpoint(a, b, lambda x, y: x / y)
```


```python
def tsum(a: Tenxor, dim: int) -> Tenxor:
    """ Sum reduction across the given dimension """
    return tredux(a, dim, 0, lambda x, y: x + y)
```


```python
def tmean(a: Tenxor, dim: int) -> Tenxor:
    """ Mean reduction across the given dimension """
    return tredux(a, dim, 0, lambda x, y: (x + y) / a.get_shape()[dim])
```

With these, let's create something more exciting, such as a batched matrix multiplication:


```python
def tmatmul(a: Tenxor, b: Tenxor) -> Tenxor:
    """ matrix multiplication along the two rightmost dimensions """
    
    # tmp[..., i, k, j] = a[..., i, k, 1] * b[..., 1, k, j]
    tmp = tmul(a.unsqueeze(-1), b.unsqueeze(-3))
    
    # res[..., i, j] = sum_k ( tmp[..., i, k, j] )
    res = tsum(tmp, -2)

    return res


assert tmatmul(
    Tenxor([1, 2, 3, 2, 4, 6], [2, 3]),
    Tenxor([1, 4, 2, 5, 3, 6], [3, 2]),
) == Tenxor([14, 32, 28, 64], shape=[2, 2])
```

And, finally, something to print our tenxors in a more faithful representation:


```python
def tenxor_to_string(t: Tenxor, dims: Optional[Sequence[int]] = None) -> str:
    """
    Prints the elements of the tensor across the given slice
    use `None` to print all items of that dimension,
    otherwise specify an index to only print items at
    that position
    """
    
    if dims is None:
        dims = [None] * len(t.get_shape())
    
    cursor = []  # current position in the tenxor
    slice_dims = []  # dimensions that span the slice we want to print
    for i, p in enumerate(dims):
        if p is None:
            cursor.append(0)
            slice_dims.append(i)
        else:
            cursor.append(p)
    
    rep = []
    has_more = True
    while has_more:
        rep.append('%7.2f  ' % t.get_at_position(cursor))
        
        # find the next item that we need to print
        # this is similar to next_position above,
        # except that this time we can only move
        # in a restricted set of dimensions
        # (i.e., those that we want to print)
        k = len(slice_dims) - 1
        cursor[slice_dims[k]] += 1
        while k > 0 and cursor[slice_dims[k]] >= t.get_shape()[slice_dims[k]]:
            cursor[slice_dims[k]] = 0
            cursor[slice_dims[k - 1]] += 1
            k -= 1

        has_more = cursor[slice_dims[0]] < t.get_shape()[slice_dims[0]]
        
        # print an appropriate separator if we reached the end of a dimension
        # (newline to rows, dashes for 2D slices, equal signs for  3D slices, etc.)
        if has_more and k != len(slice_dims) - 1:
            sep_idx = len(slice_dims) - k - 2
            rep.append([
                '\n', '\n---\n', '\n===\n', '\n***\n', '\n###\n'
            ][sep_idx])

    rep.append('\n')
    return ''.join(rep)


def tprint(t: Tenxor, dims: Optional[Sequence[int]] = None) -> str:
    print(tenxor_to_string(t, dims))
    
    
# print all items whose first index is 3 and last index is 2
# in python notation, that would be tenxor[3, :, :, :, :, 2]
tprint(trandom(7, 2, 3, 4, 8, 3, seed=22), (3, None, None, None, None, 2))
```

       0.50    -0.58     0.52    -0.77     0.44    -0.88    -0.38    -0.89  
      -0.66     0.56     0.16     0.91    -0.59     0.57     0.16    -0.10  
       0.77    -0.39     0.39    -0.23    -0.47    -0.66    -0.97     0.68  
      -0.00     0.66     0.38     0.39    -0.19    -0.12     0.47     0.72  
    ---
      -0.04     0.08     0.28     0.48    -1.00    -0.92     0.04    -0.39  
       0.38     0.18     0.73     0.41     0.63    -0.03     0.41     0.82  
      -0.90     0.11     0.22    -0.00     0.41    -0.25     0.12    -0.93  
      -0.71    -0.10     0.85     0.51    -0.31    -0.66     0.44     0.96  
    ---
       0.81    -0.23     0.09    -0.67    -0.07    -0.68    -0.66    -0.42  
       0.36     0.53     0.51     0.70    -0.49    -0.66    -0.90     0.40  
       0.80    -0.93     0.17     0.87     0.98    -0.86     0.72    -0.90  
      -0.24    -0.55    -0.26     0.29     0.00    -0.50     0.84     0.93  
    ===
      -0.27     0.72    -0.37     0.92     0.76     0.09    -0.44     0.34  
       0.96     0.67     0.77    -0.75     0.66    -0.65     0.72    -0.84  
      -0.75    -0.87    -0.25    -0.15    -0.52     0.24    -0.43     0.96  
      -0.01     0.87     0.33     0.92    -0.59    -0.77     0.19    -0.10  
    ---
       0.43     0.66     0.06    -0.06    -0.17    -0.77     0.02     0.32  
      -0.40    -0.26     0.84     0.52    -0.67     0.36    -0.41     0.74  
       0.74    -0.17     0.81     0.83     0.51     0.26    -0.49    -0.76  
      -0.54    -0.75    -0.74     0.93    -0.49     0.46    -0.83     0.74  
    ---
      -0.30    -0.30    -0.80     0.91    -0.81    -0.38    -0.11    -0.13  
       0.67    -0.41    -0.16    -0.42     0.87    -0.37     0.70     0.53  
       0.75    -0.86     0.06    -0.13    -0.60     0.13    -0.84     0.52  
       0.09     0.30    -0.89     0.24     0.01    -0.17    -0.05    -0.21  
    


The last two dimensions are printed as normal matrices, and different separators are used to separate higher order "slices".
The way to understand this is to stack the first three matrices and the last three matrices separated by `---` into two cubes, then stack the two cubes separated by `===` into a four dimensional "cube".

Sometimes I wish numpy and pytorch printed tensors in this way... ;)

## Real-world example

As promised, despite the relative simplicity of our tensor library, we can already achieve some pretty interesting applications. As a demonstration, we are going to implement a fully batched version of the graph attention layer v2 (GATv2, arxiv: [2105.14491](https://arxiv.org/abs/2105.14491)).

The graph attention layer is used to make neural networks that can learn from graph data such as molecules, social networks, and so on. It is defined by the following equations:

$$
\mathbf{x}^{\prime}_{bi} = \frac{1}{H}\sum_h \left[
\alpha_{biih}\mathbf{\Theta}_{s}\mathbf{x}_{bi} +
\sum_{j \in \mathcal{N}(i)}
\alpha_{bijh}\mathbf{\Theta}_{t}\mathbf{x}_{bj}
\right]
$$

$$
\alpha_{bijh} =
\frac{
\exp\left(\mathbf{a}_h^{\top}\mathrm{LeakyReLU}\left(
\mathbf{\Theta}_{s} \mathbf{x}_i + \mathbf{\Theta}_{t} \mathbf{x}_j
\right)\right)}
{\sum_{k \in \mathcal{N}(i) \cup \{ i \}}
\exp\left(\mathbf{a}_h^{\top}\mathrm{LeakyReLU}\left(
\mathbf{\Theta}_{s} \mathbf{x}_i + \mathbf{\Theta}_{t} \mathbf{x}_k
\right)\right)}
$$

Where $\mathbf{x}$ denotes node features, $\mathbf{\Theta}$ denote two weight matrices to extract node features, and $\textbf{a}$ denotes the attention weights.
This layer uses an attention mechanism with $H$ separate heads to aggregate information from the neighbors $j\in N(i)$ of each node $i$.
Based on the edges $e_{ij}$ of the graph, the layer uses separate matrices to transform the features of the source and target nodes.
Because we want to provide an efficient imlementation (and a sufficiendly elaborate example), the indices also differentiate different examples $b$ within the same batch.

### Input preparation

For this example, we are going to use random weights, random node features, random edges, and random number of nodes in each graph of the batch.
Let's start by defining the ranges for each index:


```python
B = 9   # nine examples in the batch
N = 7   # at most seven nodes in each graph
DI = 3  # three features for each node
DA = 4  # four features for the attention
H = 2   # two attention heads
```

Let's now sample the number of nodes for each graph, and create a binary matrix determining whether each node in each sample is present in the graph or padded:


```python
rng = random.Random(123)
num_nodes = [rng.randint(3, N) for _ in range(B)]

# node_mask[b, i] = 1 iff graph b has at least n nodes
node_mask = Tenxor(
    [
        float(i < m) for m in num_nodes
        for i in range(N)
    ], shape=[B, N]
)
```


```python
tprint(node_mask)
```

       1.00     1.00     1.00     0.00     0.00     0.00     0.00  
       1.00     1.00     1.00     1.00     1.00     0.00     0.00  
       1.00     1.00     1.00     0.00     0.00     0.00     0.00  
       1.00     1.00     1.00     1.00     1.00     1.00     0.00  
       1.00     1.00     1.00     1.00     1.00     0.00     0.00  
       1.00     1.00     1.00     0.00     0.00     0.00     0.00  
       1.00     1.00     1.00     0.00     0.00     0.00     0.00  
       1.00     1.00     1.00     1.00     1.00     1.00     0.00  
       1.00     1.00     1.00     1.00     1.00     1.00     1.00  
    


So that the first graph in the batch has three nodes, the second five, and so on.

We wow sample random features for all real nodes, while keeping features of "padding" nodes to zero.
In a real application, these features would come from the previous layer, or be the network inputs.


```python
# random features for unmasked nodes
x_node = trandom(B, N, DI, seed=98734)

# set features of padding nodes to zero
x_node = tmul(x_node, node_mask.unsqueeze(-1))
```

We can verify that only the first five nodes of the second graph have non-zero features:


```python
tprint(x_node, [1, None, None])
```

       0.62    -0.72    -0.50  
       0.29     0.64     0.35  
       0.84     0.89    -0.91  
      -0.99     0.26     0.75  
       0.54     0.86    -0.99  
      -0.00     0.00     0.00  
       0.00     0.00     0.00  
    


We also create a similar mask for edges, such that an edge is masked out if it connects padding nodes.
Note how we use take advantage of broadcasting rules to perform an outer product between the node mask and itself:


```python
# edge_mask[b, i, j] = node_mask[b, i] * node_mask[b, j]
edge_mask = tmul(
    node_mask.unsqueeze(1),
    node_mask.unsqueeze(2)
)

assert edge_mask.get_shape() == [B, N, N]
```

The edge mask is essentially a matrix of ones for all edges that connect existing nodes.
For the second graph in the batch, having five nodes, we have a 5x5 matrix of ones plus two rows and columns of zeros for the padding nodes:


```python
tprint(edge_mask, (1, None, None))
```

       1.00     1.00     1.00     1.00     1.00     0.00     0.00  
       1.00     1.00     1.00     1.00     1.00     0.00     0.00  
       1.00     1.00     1.00     1.00     1.00     0.00     0.00  
       1.00     1.00     1.00     1.00     1.00     0.00     0.00  
       1.00     1.00     1.00     1.00     1.00     0.00     0.00  
       0.00     0.00     0.00     0.00     0.00     0.00     0.00  
       0.00     0.00     0.00     0.00     0.00     0.00     0.00  
    


And here we randomly create the actual adjacency matrix for each graph.
In real applications, this is given as input to the network.


```python
adjacency = trandom(B, N, N,seed=235).transform_values(lambda x: float(x > 0))
adjacency = tmul(edge_mask, adjacency)
```

For the second graph in the batch, we have:


```python
tprint(adjacency, (1, None, None))
```

       1.00     0.00     0.00     0.00     1.00     0.00     0.00  
       1.00     0.00     0.00     1.00     0.00     0.00     0.00  
       1.00     0.00     1.00     0.00     1.00     0.00     0.00  
       0.00     1.00     0.00     1.00     1.00     0.00     0.00  
       0.00     0.00     0.00     0.00     0.00     0.00     0.00  
       0.00     0.00     0.00     0.00     0.00     0.00     0.00  
       0.00     0.00     0.00     0.00     0.00     0.00     0.00  
    


To properly compute attention and node aggregation, we need to differentiate between self-connections and connections with other nodes, which we will do with two different adjacency matrices.


```python
diag = Tenxor([
    float(i == j)
    for _ in range(B)
    for i in range(N)
    for j in range(N)
], shape=[B, N, N])

# remove self-connections by multiplying with 
# zeros on the diagonal and ones everywhere else
adjacency_no_self = tmul(
    adjacency, diag.transform_values(lambda x: 1 - x)
)

# now add self-connections for all nodes
adjacency_self = tmul(
    tadd(
        adjacency_no_self, diag
    ),
    edge_mask
)

assert adjacency_no_self.get_shape() == adjacency_self.get_shape() == [B, N, N]
```

In a real-world implementation, it would be much more efficient to rely on indexing to select appropriate nodes, but since we have not implemented this feature we are going to use matrix multiplications with binary matrices.

### Model parameters

The model parameters are also samped randomly.
For real applications, random initialization that makes sense is a non-trivial (but mostly solved) affair, but here we don't need to worry too much about it:


```python
attention_params = trandom(DA, H, seed=2398)
theta_source = trandom(DI, DA, seed=9245)
theta_target = trandom(DI, DA, seed=85745)
```

### Attention

We can finally start with the computations!
Here's once again the formula for the attention weights for head $h$ between nodes $i$ and $j$ of the $b$-th graph:

$$
\alpha_{bijh} =
\frac{
\exp\left(\mathbf{a}_h^{\top}\mathrm{LeakyReLU}\left(
\mathbf{\Theta}_{s} \mathbf{x}_i + \mathbf{\Theta}_{t} \mathbf{x}_j
\right)\right)}
{\sum_{k \in \mathcal{N}(i) \cup \{ i \}}
\exp\left(\mathbf{a}_h^{\top}\mathrm{LeakyReLU}\left(
\mathbf{\Theta}_{s} \mathbf{x}_i + \mathbf{\Theta}_{t} \mathbf{x}_k
\right)\right)}
$$


```python
x_source = tmatmul(x_node, theta_source)
x_target = tmatmul(x_node, theta_target)

assert x_source.get_shape() == x_target.get_shape() == [B, N, DA]
```

Let's now compute the attention logits before the softmax, starting from $\text{LeakyReLU}(\mathbf{\Theta_s}\textbf{x}_{bi}+\mathbf{\Theta_t}\textbf{x}_{bj})$ which we call `z_inner` in the code:


```python
# z_inner[b, i, j, da] = leaky_relu ( x_source[b, i, da] * x_target[b, j, da] )
z_inner = tadd(
    x_source.unsqueeze(2),
    x_target.unsqueeze(1),
).transform_values(lambda x: x if x >= 0 else x * 0.01)

assert z_inner.get_shape() == [B, N, N, DA]
```

Now we let each attention head look at these interactions:


```python
# zi[b, i, j, h] = sum_k ( zi[b, i, j, k] * attention_params[k, h] )
z_attention = tmatmul(z_inner, attention_params)

assert z_attention.get_shape() == [B, N, N, H]
```

Here is, for example, the logits for the first attention head on the second graph in the batch:


```python
tprint(z_attention, (1, None, None, 0))
```

      -0.61     1.07     0.79     0.29     0.53     0.14     0.14  
      -0.32     0.33     0.35     0.05     0.14     0.01     0.01  
      -0.58     0.21     0.10     0.07    -0.20    -0.07    -0.07  
      -0.12     0.49     0.57     0.13     0.41     0.10     0.10  
      -0.57     0.21     0.09     0.08    -0.21    -0.06    -0.06  
      -0.49     0.66     0.65     0.04     0.39     0.00     0.00  
      -0.49     0.66     0.65     0.04     0.39     0.00     0.00  
    


We should now normalize each row in the interval (0, 1) through the softmax.
However, we cannot use the function we wrote above as we now have some elements that should be ignored as they do not corresponds to real nodes in the graph.
With a feature-complete library such as pytorch we could do this more efficiently by indexing directly the nodes that we want to use, but since we did not implement indexing (yet) we have to use the binary masks we computed earlier to ignore the padding nodes.

This masked softmax is therefore computed in two steps.
First is the numerator, where we use the edge mask to set the weights of padding nodes to zero. Here we use the adjacency matrix with forced self-connections, so that nodes are allowed to attend to themselves:


```python
z_attention_exp = z_attention.transform_values(math.exp)

# zen[b, i, j, h] = adjacency[b, i, j] * za[b, i, j, h]
z_attention_exp_numerator = tmul(adjacency_self.unsqueeze(-1), z_attention_exp)

assert z_attention_exp_numerator.get_shape() == [B, N, N, H]
```


```python
tprint(z_attention_exp_numerator, (1, None, None, 0))
```

       0.54     0.00     0.00     0.00     1.71     0.00     0.00  
       0.72     1.39     0.00     1.05     0.00     0.00     0.00  
       0.56     0.00     1.10     0.00     0.82     0.00     0.00  
       0.00     1.63     0.00     1.14     1.50     0.00     0.00  
       0.00     0.00     0.00     0.00     0.81     0.00     0.00  
       0.00     0.00     0.00     0.00     0.00     0.00     0.00  
       0.00     0.00     0.00     0.00     0.00     0.00     0.00  
    


Next the denominator, where we must to consider self-connections for the normalization:


```python
z_attention_exp_denominator = tsum(
    tmul(adjacency_self.unsqueeze(-1), z_attention_exp),
    dim=2
)

assert z_attention_exp_denominator.get_shape() == [B, N, H]
```

Here's the total weight given to each node of the second graph.
This is what we use to normalize the attention weights between zero and one:


```python
tprint(z_attention_exp_denominator, (1, None, 0))
```

       2.25     3.17     2.48     4.28     0.81     0.00     0.00  
    


And here's the full attention tensor after normalization:


```python
# alpha[b, i, j, h] = (
#   z_attention_exp_numerator[b, i, j, h] / z_attention_exp_denominator[b, i, h]
# )
# divisions by zero happen for padding nodes, so we set the attention weight to zero
alpha = tpoint(
    z_attention_exp_numerator,
    z_attention_exp_denominator.unsqueeze(2),
    lambda x, y: x / y if y != 0 else 0
)

assert alpha.get_shape() == [B, N, N, H]
```

Here's the attention weights that the first head uses on the second graph:


```python
tprint(alpha, (1, None, None, 0))
```

       0.24     0.00     0.00     0.00     0.76     0.00     0.00  
       0.23     0.44     0.00     0.33     0.00     0.00     0.00  
       0.23     0.00     0.44     0.00     0.33     0.00     0.00  
       0.00     0.38     0.00     0.27     0.35     0.00     0.00  
       0.00     0.00     0.00     0.00     1.00     0.00     0.00  
       0.00     0.00     0.00     0.00     0.00     0.00     0.00  
       0.00     0.00     0.00     0.00     0.00     0.00     0.00  
    


Note how each row sums to one and the only positive weights are for actual graph nodes and their neighbors.
For reference, here's the adjacency matrix of this graph:


```python
tprint(adjacency_self, (1, None, None))
```

       1.00     0.00     0.00     0.00     1.00     0.00     0.00  
       1.00     1.00     0.00     1.00     0.00     0.00     0.00  
       1.00     0.00     1.00     0.00     1.00     0.00     0.00  
       0.00     1.00     0.00     1.00     1.00     0.00     0.00  
       0.00     0.00     0.00     0.00     1.00     0.00     0.00  
       0.00     0.00     0.00     0.00     0.00     0.00     0.00  
       0.00     0.00     0.00     0.00     0.00     0.00     0.00  
    


### Aggregation

Having computed the weights for the attention, we can now use them for the node aggregation, which is computed as follows:

$$
\mathbf{x}^{\prime}_{bi} = \frac{1}{H}\sum_h \left[
\alpha_{biih}\mathbf{\Theta}_{s}\mathbf{x}_{bi} +
\sum_{j \in \mathcal{N}(i)}
\alpha_{bijh}\mathbf{\Theta}_{t}\mathbf{x}_{bj}
\right]
$$

We start by computing the inner sum, that aggregates the neighbors of each node.
First, let's broadcast their features and weights: 


```python
x_target_expanded = x_target.unsqueeze(2).unsqueeze(3)
assert x_target_expanded.get_shape() == [B, N, 1, 1, DA]

alpha_expanded = alpha.unsqueeze(-1)
assert alpha_expanded.get_shape() == [B, N, N, H, 1]
```

Let us now scale the features of each node with the appropriate attention weight:


```python
# x_incoming_each_neighbor[b, i, j, h, da] = (
#   x_target_expanded[b, i, da] * alpha_expanded[b, i, j, h]
# )
x_incoming_each_neighbor = tmul(x_target_expanded, alpha_expanded)

assert x_incoming_each_neighbor.get_shape() == [B, N, N, H, DA]
```

With respect to the formula above, `x_incoming_each_neighbor` contains $\alpha_{bijh}\mathbf{\Theta}_t\textbf{x}_{bj}$.

Note that the attention weights were computed with forced self-loops, but to compute the incoming messages we should only consider weights of the neighbor nodes as given by the input adjacency (which may or may not include self-loops):


```python
# x_incoming_neighbors[b, i, j, h, da] = (
#   x_incoming_neighbors[b, i, j, h, da] * adjacency[b, i, j]
# )
x_incoming_neighbors = tmul(
    x_incoming_each_neighbor,
    adjacency.unsqueeze(-1).unsqueeze(-1),
)

assert x_incoming_neighbors.get_shape() == [B, N, N, H, DA]
```

We can now sum all incoming messages:


```python
# x_incoming_neighbors[b, i, h, da] = sum_j ( x_incoming_neighbors[b, i, j, h, da] )
x_incoming_neighbors = tsum(x_incoming_neighbors, dim=2)

assert x_incoming_neighbors.get_shape() == [B, N, H, DA]
```

To that we add the result of self-attention. By using the identity we can isolate the respective weight:


```python
# x_self_attention[b, i, j, h] = diag[b, i, j] * alpha[b, i, j, h]
x_self_attention = tmul(
    diag.unsqueeze(-1),
    alpha,
)

assert x_self_attention.get_shape() == [B, N, N, H]
```

And multiply it with the source features:


```python
# x_incoming_self[b, i, j, h, da] = x_source[b, i, da] * alpha[b, i, i, h]
x_incoming_self = tmul(
    x_source.unsqueeze(2).unsqueeze(3),
    x_self_attention.unsqueeze(-1),
)

x_incoming_self = tsum(x_incoming_self, 2)

assert x_incoming_self.get_shape() == [B, N, H, DA]
```

Here's where we are now:

$$
\mathbf{x}^{\prime}_{bi} = \frac{1}{H}\sum_h \left[
\underbrace{
\alpha_{biih}\mathbf{\Theta}_{s}\mathbf{x}_{bi}
}_{\text{x\_incoming\_self}}+
\underbrace{
\sum_{j \in \mathcal{N}(i)}
\alpha_{bijh}\mathbf{\Theta}_{t}\mathbf{x}_{bj}
}_{\text{x\_incoming\_neighbors}}
\right]
$$

Lastly, we add the self-attention with the incoming messages, and average across the attention heads:


```python
# x_out[b, n, h, da] = (
#   x_incoming_self[b, n, h, da] +  x_incoming_neighbors[b, n, h, da]
# )
x_out = tadd(x_incoming_self, x_incoming_neighbors)

# x_out[b, n, da] = mean_h ( x_out[b, n, h, da] )
x_out = tmean(x_out, 2)

assert x_out.get_shape() == [B, N, DA]
```

And here's how to implement a graph neural network, just stack a few of these!

## Axis re-ordering

What we wrote so far is already pretty freakin' cool, if you ask me, but it is still missing some core features.
One of these is the ability to re-order axes, which is very useful to compute, for example, the transpose of a matrix.

To implement this, let's create a new tensor class that encapsulates another tensor, and permutes the axes in a desired order:


```python
class TransposedTenxor(AbstractTenxor):
    def __init__(self, wrapped: AbstractTenxor, axis_order: List[int]):
        self._wrapped = wrapped
        
        # the given order should contain a permutation
        # of the numbers 0, ..., len(shape) - 1
        if (len(axis_order) != len(wrapped.get_shape()) 
                or set(axis_order) != set(range(len(wrapped.get_shape())))):
            raise ValueError(
                f"given axis order {axis_order} incompatible"
                f"with shape {self._wrapped.get_shape()}"
            )
        self._axis_order = axis_order
            
    def get_shape(self) -> List[int]:
        shape = self._wrapped.get_shape()
        return [shape[i] for i in self._axis_order]
    
    def get_at_position(self, pos: List[int]) -> float:
        return self._wrapped.get_at_position([pos[i] for i in self._axis_order])
    
    def set_at_position(self, pos: List[int], val: float) -> None:
        self._wrapped.set_at_position([pos[i] for i in self._axis_order], val)

    def next_position(self, pos: List[int]) -> bool:
        return next_position(pos, self.get_shape())

    def transform_values(self, op: Callable[[float], float]) -> 'AbstractTenxor':
        return self._wrapped.transform_values(op)
```


```python
def tswapaxes(tenxor: AbstractTenxor, ax1: int, ax2: int):
    """ Swaps the two given axes of a tensor. """
    order = list(range(len(tenxor.get_shape())))
    order[ax1], order[ax2] = order[ax2], order[ax1]
    return TransposedTenxor(tenxor, order)


def ttranspose(tenxor: AbstractTenxor):
    """ Transposes the tensor, i.e., swaps the last two axes. """
    return tswapaxes(tenxor, -1, -2)
```


```python
tw = Tenxor([1, 2, 3, 4, 5, 6], [2, 3])
tprint(tw)
```

       1.00     2.00     3.00  
       4.00     5.00     6.00  
    



```python
tt = ttranspose(tw)
tprint(tt)
```

       1.00     4.00  
       2.00     5.00  
       3.00     6.00  
    



```python
ttt = ttranspose(tt)
tprint(ttt)
```

       1.00     2.00     3.00  
       4.00     5.00     6.00  
    


## Reshaping redefined

The current implementation of reshaping is easy enough, we simply overwrite the old shape with the new shape and everything works out of the box.
Unfortunately, this does not work anymore with our transposed tensor.

To support generic reshaping, we apply the same trick that we used to swap axes: we will provide a "translation" layer that will match locations before and after reshaping.
This conversion is actually quite simple: since both tensors refer to the same underlying storage, the two positions must correspond to the same element in that storage.
We can therefore find that element based on the position and shape of the reshaped tensor, then convert the offset of that element into the position of the element with the same offset that is in the original tensor.
It's a mouthful in English, but very simple in code:


```python
def view_position_to_original_position(
    view_pos: List[int], view_shape: List[int], original_shape: List[int]
) -> List[int]:
    """ Converts the position of an element in a reshaped tensor
        to the position of the same element in the original tensor.
    """
    
    offset = position_to_index(view_pos, view_shape)
    pos = index_to_position(offset, original_shape)
    return pos


assert view_position_to_original_position([0, 1], [3, 2], [6]) == [1]
assert view_position_to_original_position([2, 1], [3, 2], [6]) == [5] 
```

Here's the wrapper that uses this function:


```python
class ReshapedTenxor(AbstractTenxor):
    def __init__(self, wrapped: AbstractTenxor, shape: List[int]):
        self._wrapped = wrapped
        self._shape = shape
        
    def get_shape(self) -> List[int]:
        return self._shape
    
    def get_at_position(self, pos: List[int]) -> float:
        pos = view_position_to_original_position(
            pos, self._shape, self._wrapped.get_shape()
        )
        return self._wrapped.get_at_position(pos)
    
    def set_at_position(self, pos: List[int], val: float) -> None:
        pos = view_position_to_original_position(
            pos, self._shape, self._wrapped.get_shape()
        )
        self._wrapped.set_at_position(pos, val)
        
    def next_position(self, pos) -> bool:
        return next_position(pos, self._shape)
    
    def transform_values(self, op: Callable[[float], float]) -> 'Tenxor':
        return self._wrapped.transform_values(op)
```

And here's a small test:


```python
t = Tenxor(list(range(6)), [6])
tprint(t)
```

       0.00     1.00     2.00     3.00     4.00     5.00  
    



```python
tprint(ReshapedTenxor(t, [2, 3]))
```

       0.00     1.00     2.00  
       3.00     4.00     5.00  
    



```python
tprint(ReshapedTenxor(t, [3, 2]))
```

       0.00     1.00  
       2.00     3.00  
       4.00     5.00  
    


## Indexing

The last major feature we need to support is indexing items or entire slices of a tensor, i.e., we want to write code like `matrix[2:5, 1:-1]` to take rows 2, 3 and 4 all columns but the first and last.
And, as usual, we want to do this efficiently, i.e., without creating a new copy of the underlying data storage.

We will offer several ways to subset each dimension:
 - A `None` means that we do not subset that axis;
 - An `int` means that we only take the slice with the specified index;
 - A `List[int]` means that we take all slices with those indices;
 - A generic `slice` object allows us to consider arbitrary expressions like `20:5:-3`.

As before, we will create a wrapper that will redirect the index location to the correct position of the wrapped tensor.
The first thing to implement is to use these specifications to compute the final shape of the sliced tensor:


```python
Slice = slice | List[int] | int | None


def compute_sliced_shape(full_shape: List[int], slices: List[Slice]) -> List[int]:
    """ Computes the shape that results from slicing a tensor of the given shape.
    """
    
    if len(slices) > len(full_shape):
        raise RuntimeError("specify at most one slice per dimension")
    elif len(slices) < len(full_shape):
        slices = slices + [None] * (len(full_shape) - len(slices))
    
    new_shape = []
    for dim, sli in zip(full_shape, slices):
        # find out how many items we take of this axis
        if sli is None:
            # no slicing here
            new_shape.append(dim)
        elif isinstance(sli, int):
            # only take a single element
            new_shape.append(1)
        elif isinstance(sli, list):
            # take a subset of elements
            new_shape.append(len(sli))
        elif isinstance(sli, slice):
            # find how many slices we need to take
            start, stop, step = sli.indices(dim)
            count = math.ceil((stop - start) / step)
            new_shape.append(count)
        else:
            raise RuntimeError("unsupported slicing specification")
    
    return new_shape


assert compute_sliced_shape(
    [5, 4, 3],
    # take 3rd and 4th of the first dimension, all of the second,
    # and 2nd of third dimension
    [[2, 3], None, 1]
) == [2, 4, 1]

assert compute_sliced_shape(
    [5, 4, 3],
    # take 4th of the 1st dimension, and everything else
    [3]
) == [1, 4, 3]

assert compute_sliced_shape(
    [5, 4, 3],
    # take everything of the 1st dimension, 4th and 3rd of second dimension,
    # and everything else
    [None, slice(3, 1, -1)]
) == [5, 2, 3]
```

The next step is to translate positions in the sliced tensor back to the position in the full tensor:


```python
def sliced_position_to_full_position(
    pos: List[int], full_shape: List[int], slices: List[slice]
) -> List[int]:
    """ Converts the position of an element in a sliced tensor to the 
        position of that element in the original tensor.
    """
    if len(slices) > len(full_shape):
        raise RuntimeError("specify at most one slice per dimension")
    elif len(slices) < len(full_shape):
        slices = slices + [None] * (len(full_shape) - len(slices))
    
    full_pos = []
    for idx, dim, sli in zip(pos, full_shape, slices):
        if sli is None:
            # no slicing here, the index did not change
            full_pos.append(idx)
        elif isinstance(sli, int):
            # only one dimension was taken
            # the index in the implied shape must be 0
            # and the index in the full shape is the slice that was taken
            if idx != 0:
                raise RuntimeError("out of bounds")
            full_pos.append(sli)
        elif isinstance(sli, list):
            # a subset of elements was taken
            # use the provided list to map indices back
            full_pos.append(sli[idx])
        elif isinstance(sli, slice):
            # compute which index was taken
            start, stop, step = sli.indices(dim)
            count = math.ceil((stop - start) / step)
            if idx >= count:
                raise RuntimeError("out of bounds")
            full_pos.append(start + step * idx)
        else:
            raise RuntimeError("unsupported slicing specification")
        
    return full_pos
```

Let's create a test example:


```python
full_shape = [5, 2, 3]
slices = [
    [3, 4],          # 4th and 5th of first dimension
    None,            # all of second dimension 
    slice(3, 0, -1)  # 3rd and 2nd of 3rd dimension (index 3 is out-of-bounds)
]
subset_shape = compute_sliced_shape(full_shape, slices)
subset_shape
```




    [2, 2, 2]



Now we navigate across all elements of the sliced tensor, and check that they are mapped to the correct position of the original tensor:


```python
subset_pos = [0, 0, 0]
has_more = True
while has_more:
    full_pos = sliced_position_to_full_position(subset_pos, full_shape, slices)
    print(f'position {subset_pos} maps to {full_pos}')
    has_more = next_position(subset_pos, subset_shape)
```

    position [0, 0, 0] maps to [3, 0, 2]
    position [0, 0, 1] maps to [3, 0, 1]
    position [0, 1, 0] maps to [3, 1, 2]
    position [0, 1, 1] maps to [3, 1, 1]
    position [1, 0, 0] maps to [4, 0, 2]
    position [1, 0, 1] maps to [4, 0, 1]
    position [1, 1, 0] maps to [4, 1, 2]
    position [1, 1, 1] maps to [4, 1, 1]


Note how this also handles moving backwards. Neat!

Last, let's create the wrapper:


```python
class SlicedTenxor(AbstractTenxor):
    def __init__(self, wrapped: Tenxor, slices: List[Slice]):
        self._wrapped = wrapped
        self._slices = slices
        self._shape = compute_sliced_shape(
            self._wrapped.get_shape(), self._slices
        )
        
    def position_to_index(self, pos: List[int]) -> int:
        return position_to_index(pos, self._shape)
    
    def index_to_position(self, idx: int) -> List[int]:
        return index_to_position(idx, self._shape)
    
    def next_position(self, pos: List[int]) -> bool:
        return next_position(pos, self._shape)
    
    def get_shape(self) -> List[int]:
        return self._shape
    
    def get_at_position(self, pos: List[int]) -> float:
        full_pos = sliced_position_to_full_position(
            pos, self._wrapped.get_shape(), self._slices
        )
        return self._wrapped.get_at_position(full_pos)
    
    def set_at_position(self, pos: List[int], val: float) -> None:
        full_pos = sliced_position_to_full_position(
            pos, self._wrapped.get_shape(), self._slices
        )
        self._wrapped.set_at_position(full_pos, val)

    def next_position(self, pos: List[int]) -> bool:
        return next_position(pos, self.get_shape())

    def transform_values(self, op: Callable[[float], float]) -> 'AbstractTenxor':
        return self._wrapped.transform_values(op)
```

## Putting everything together

Let's hide the tensor wrappers into some methods of the base tensor class:


```python
AbstractTenxor.view = lambda t, s: ReshapedTenxor(t, s)
AbstractTenxor.t = ttranspose
AbstractTenxor.swapaxes = tswapaxes
AbstractTenxor.slice = lambda t, s: SlicedTenxor(t, s)
AbstractTenxor.__getitem__ = AbstractTenxor.slice
AbstractTenxor.__repr__ = Tenxor.__repr__ = tenxor_to_string
```

And now, let's try to combine everything we've done so far in a pointlessly convoluted example!


```python
t1 = Tenxor(list(range(20)), [4, 5])
t1
```




       0.00     1.00     2.00     3.00     4.00  
       5.00     6.00     7.00     8.00     9.00  
      10.00    11.00    12.00    13.00    14.00  
      15.00    16.00    17.00    18.00    19.00  




```python
a = t1.slice([[1, 3], slice(0, 5, 2)])
a
```




       5.00     7.00     9.00  
      15.00    17.00    19.00  




```python
at = a.t()
at
```




       5.00    15.00  
       7.00    17.00  
       9.00    19.00  



Note that our implementation also allows to duplicate dimensions, isn't that cool?


```python
b = t1.slice([[1, 1], slice(4)])
b
```




       5.00     6.00     7.00     8.00  
       5.00     6.00     7.00     8.00  




```python
bt = b.swapaxes(0, 1)
bt
```




       5.00     5.00  
       6.00     6.00  
       7.00     7.00  
       8.00     8.00  




```python
tr = tmatmul(at, bt.view([2, 4]))
tr
```




     130.00   130.00   150.00   150.00  
     154.00   154.00   178.00   178.00  
     178.00   178.00   206.00   206.00  



I have no idea what this is supposed to be, but it didn't crash so I count it as a success :)

# Conclusion

At first sight tensors may feel like mystical objects, but in reality you only need a few simple basic routines to open the door to a wide range of operations.
Using wrappers to translate coordinates between different versions of a tensor is an especially elegant approach that enables some crazy complicated sequences of transformations in such a simple and beautiful way.

The best blog posts are those where at the end you realize that climbed up a mountain and learned things you believed were out of your reach before staring.
This was definitely one of those for me, and I hope you found it useful as well.
