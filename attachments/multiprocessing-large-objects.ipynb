{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "layout: \"post\"\n",
    "title: \"Using large numpy arrays and pandas dataframes with multiprocessing\"\n",
    "date: 2020-06-19 09:00:00 +0200\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks to multiprocessing, it is relatively straightforward to write parallel\n",
    "code in Python. However, these processes communicate by copying and\n",
    "(de)serializing data, which can make parallel code even slower when large\n",
    "objects are passed back and forth. This post shows how to use shared memory to\n",
    "avoid all the copying and serializing, making it possible to have fast parallel\n",
    "code that works with large datasets.\n",
    "\n",
    "<!-- more -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to demonstrate the problem empirically, let us create a large data-frame and do some processing on each row:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size: 32.4 MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Col-0</th>\n",
       "      <th>Col-1</th>\n",
       "      <th>Col-2</th>\n",
       "      <th>Col-3</th>\n",
       "      <th>Col-4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Idx-0</th>\n",
       "      <td>0.980677</td>\n",
       "      <td>0.921510</td>\n",
       "      <td>0.910434</td>\n",
       "      <td>0.927914</td>\n",
       "      <td>0.222692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Idx-1</th>\n",
       "      <td>0.320717</td>\n",
       "      <td>0.412364</td>\n",
       "      <td>0.007833</td>\n",
       "      <td>0.874941</td>\n",
       "      <td>0.121518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Idx-2</th>\n",
       "      <td>0.219621</td>\n",
       "      <td>0.400342</td>\n",
       "      <td>0.823636</td>\n",
       "      <td>0.178868</td>\n",
       "      <td>0.322418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Idx-3</th>\n",
       "      <td>0.577504</td>\n",
       "      <td>0.622186</td>\n",
       "      <td>0.218873</td>\n",
       "      <td>0.142106</td>\n",
       "      <td>0.871804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Idx-4</th>\n",
       "      <td>0.590041</td>\n",
       "      <td>0.533683</td>\n",
       "      <td>0.004371</td>\n",
       "      <td>0.599954</td>\n",
       "      <td>0.178846</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Col-0     Col-1     Col-2     Col-3     Col-4\n",
       "Idx-0  0.980677  0.921510  0.910434  0.927914  0.222692\n",
       "Idx-1  0.320717  0.412364  0.007833  0.874941  0.121518\n",
       "Idx-2  0.219621  0.400342  0.823636  0.178868  0.322418\n",
       "Idx-3  0.577504  0.622186  0.218873  0.142106  0.871804\n",
       "Idx-4  0.590041  0.533683  0.004371  0.599954  0.178846"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows, cols = 1000, 5000\n",
    "df = pd.DataFrame(\n",
    "    np.random.random(size=(rows, cols)),\n",
    "    columns=[f'Col-{i}' for i in range(cols)],\n",
    "    index=[f'Idx-{i}' for i in range(rows)]\n",
    ")\n",
    "\n",
    "print(f'Data size: {df.values.nbytes / 1024 / 1204:.1f} MB')\n",
    "df.iloc[:5, :5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following is a simple, and a bit silly, row transformation: we construct a random matrix of the same size as the dataframe, take the mean across rows, and compute the outer product between it and the specified row. It is not too heavy computationally, but (this is the important part), both its inputs and outputs are matrices of size $1000\\times5000$, which have five millions entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_work(args):\n",
    "    df, idx = args\n",
    "    data = np.random.random(size=(len(df), len(df.columns)))\n",
    "    result = np.outer(df.loc[idx], data.mean(axis=0))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first set a baseline by transforming 250 random rows in a sequential fashion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 250/250 [00:16<00:00, 15.08it/s]\n"
     ]
    }
   ],
   "source": [
    "process_rows = np.random.choice(len(df), 250)\n",
    "for i in tqdm(process_rows):\n",
    "    result = do_work((df, df.index[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how you would naively transform this code to a parallel version using multiprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 250/250 [02:37<00:00,  1.58it/s]\n"
     ]
    }
   ],
   "source": [
    "with mp.Pool() as pool:\n",
    "    tasks = ((df, df.index[idx]) for idx in process_rows)\n",
    "    result = pool.imap(do_work, tasks)\n",
    "    for res in tqdm(result, total=len(process_rows)):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is much slower! As I hinted above, the problem is that the processes are exchanging a lot of data that has to be serialized, copied, and de-serialized. All of this takes time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thansk to [`shared_memory`](https://docs.python.org/3/library/multiprocessing.shared_memory.html), making this fast is a breeze! A caveat, though: it only works with Python 3.8 or above.\n",
    "\n",
    "We are first going to deal with plain numpy arrays, then build upon this to share pandas dataframes. The idea is to write a wrapper that takes care of moving data to and from the shared memory. I strongly encourage you to read the documentation I linked above, which explains this in more detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing.shared_memory import SharedMemory\n",
    "\n",
    "\n",
    "class SharedNumpyArray:\n",
    "    '''\n",
    "    Wraps a numpy array so that it can be shared quickly among processes,\n",
    "    avoiding unnecessary copying and (de)serializing.\n",
    "    '''\n",
    "    def __init__(self, array):\n",
    "        '''\n",
    "        Creates the shared memory and copies the array therein\n",
    "        '''\n",
    "        # create the shared memory location of the same size of the array\n",
    "        self._shared = SharedMemory(create=True, size=array.nbytes)\n",
    "        \n",
    "        # save data type and shape, necessary to read the data correctly\n",
    "        self._dtype, self._shape = array.dtype, array.shape\n",
    "        \n",
    "        # create a new numpy array that uses the shared memory we created.\n",
    "        # at first, it is filled with zeros\n",
    "        res = np.ndarray(\n",
    "            self._shape, dtype=self._dtype, buffer=self._shared.buf\n",
    "        )\n",
    "        \n",
    "        # copy data from the array to the shared memory. numpy will\n",
    "        # take care of copying everything in the correct format\n",
    "        res[:] = array[:]\n",
    "\n",
    "    def read(self):\n",
    "        '''\n",
    "        Reads the array from the shared memory without unnecessary copying.\n",
    "        '''\n",
    "        # simply create an array of the correct shape and type,\n",
    "        # using the shared memory location we created earlier\n",
    "        return np.ndarray(self._shape, self._dtype, buffer=self._shared.buf)\n",
    "\n",
    "    def copy(self):\n",
    "        '''\n",
    "        Returns a new copy of the array stored in shared memory.\n",
    "        '''\n",
    "        return np.copy(self.read_array())\n",
    "        \n",
    "    def close(self):\n",
    "        '''\n",
    "        Closes the shared memory. Call when this object will not\n",
    "        be used anymore *in this process* (but it can still be used \n",
    "        in other processes)\n",
    "        '''\n",
    "        self._shared.close()\n",
    "    \n",
    "    def unlink(self):\n",
    "        '''\n",
    "        Unlinks the shared memory. Call when this object will not be\n",
    "        used anymore in any process.\n",
    "        '''\n",
    "        self._shared.unlink()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, use this class to wrap the array, and send this wrapped object as parameter to a process and/or as a return value of the process, it's that simple! \n",
    "\n",
    "In particular, note that the array itself is not saved in the object. That is the whole point, we do not want to move it around! We can move the shared memory, though, because doing so will not copy the underlying memory, only a reference to it will be moved.\n",
    "\n",
    "**A brief note on memory management:** as you can see, there is a `close` method and an `unlink` method: they must be used correctly so that the runtime can reclaim the memory used by objects that are not needed anymore. Failure of doing so will result in increasing memory usage over time, until no more free memory is available and the program will crash. How to avoid this? Briefly, every process has to `close` shared memory objects as soon as they are not needed anymore, and the last process *in addition* has to `unlink` it. See the producer-consumer example below for an example of this.\n",
    "\n",
    "Using that wrapper, it is trivial to share a pandas dataframe: we wrap the values using the class above, and save index and columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SharedPandasDataFrame:\n",
    "    '''\n",
    "    Wraps a pandas dataframe so that it can be shared quickly among processes,\n",
    "    avoiding unnecessary copying and (de)serializing.\n",
    "    '''\n",
    "    def __init__(self, df):\n",
    "        '''\n",
    "        Creates the shared memory and copies the dataframe therein\n",
    "        '''\n",
    "        self._values = SharedNumpyArray(df.values)\n",
    "        self._index = df.index\n",
    "        self._columns = df.columns\n",
    "\n",
    "    def read(self):\n",
    "        '''\n",
    "        Reads the dataframe from the shared memory\n",
    "        without unnecessary copying.\n",
    "        '''\n",
    "        return pd.DataFrame(\n",
    "            self._values.read(),\n",
    "            index=self._index,\n",
    "            columns=self._columns\n",
    "        )\n",
    "    \n",
    "    def copy(self):\n",
    "        '''\n",
    "        Returns a new copy of the dataframe stored in shared memory.\n",
    "        '''\n",
    "        return pd.DataFrame(\n",
    "            self._values.copy(),\n",
    "            index=self._index,\n",
    "            columns=self._columns\n",
    "        )\n",
    "            \n",
    "    def close(self):\n",
    "        '''\n",
    "        Closes the shared memory. Call when this object will not\n",
    "        be used anymore *in this process* (but it can still be used \n",
    "        in other processes)\n",
    "        '''\n",
    "        self._values.close()\n",
    "    \n",
    "    def unlink(self):\n",
    "        '''\n",
    "        Unlinks the shared memory. Call when this object will not be\n",
    "        used anymore in any process.\n",
    "        '''\n",
    "        self._values.unlink()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is how to use them, and how quick it is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def work_fast(args):\n",
    "    shared_df, idx = args\n",
    "    \n",
    "    # read dataframe from shared memory\n",
    "    df = shared_df.read()\n",
    "    \n",
    "    # call old function\n",
    "    result = do_work((df, idx))\n",
    "    \n",
    "    # wrap and return the result\n",
    "    return SharedNumpyArray(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 250/250 [00:21<00:00, 11.50it/s]\n"
     ]
    }
   ],
   "source": [
    "shared_df = SharedPandasDataFrame(df)\n",
    "\n",
    "with mp.Pool() as pool:\n",
    "    tasks = ((shared_df, df.index[idx]) for idx in process_rows)\n",
    "    result = pool.imap(work_fast, tasks)\n",
    "    for res in tqdm(result, total=len(process_rows)):\n",
    "        res.unlink()  # IMPORTANT\n",
    "\n",
    "shared_df.unlink()  # IMPORTANT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wait a minute! you might say, this is barely faster than the single-process version! Well yes, but it is roughly seven times faster than the multiprocessing version :). You might have noticed that there is still some copying going on, after all: when we create the shared memory, we have to copy the array in there. Depending on the computations you perform in the worker process, you might be able to avoid this, e.g. by pre-allocating the shared memory and performing only in-place operations, but it strongly depends on exactly what and how you compute.\n",
    "\n",
    "The advantage of multiprocessing with shared memory becomes more apparent when workers perform more computations. For example, suppose we want to take the convolution of the whole dataframe with a $20\\times20$ filter made by $20^2$ random entries of the specified row:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage import convolve\n",
    "\n",
    "def do_work(args):\n",
    "    df, idx = args\n",
    "    kernel_idx = np.random.choice(df.shape[1], 20 * 20)\n",
    "    kernel = df.loc[idx][kernel_idx].values.reshape((20, 20))\n",
    "    result = convolve(df.values, kernel)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, let us compare the sequential and multi-process versions. We can safely rule out the naive multiprocessing solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 24/24 [01:01<00:00,  2.55s/it]\n"
     ]
    }
   ],
   "source": [
    "process_rows = np.random.choice(len(df), 24)\n",
    "for i in tqdm(process_rows):\n",
    "    result = do_work((df, df.index[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 24/24 [00:16<00:00,  1.43it/s]\n"
     ]
    }
   ],
   "source": [
    "shared_df = SharedPandasDataFrame(df)\n",
    "\n",
    "with mp.Pool() as pool:\n",
    "    tasks = ((shared_df, df.index[idx]) for idx in process_rows)\n",
    "    result = pool.imap(work_fast, tasks)\n",
    "    for res in tqdm(result, total=len(process_rows)):\n",
    "        res.unlink()  # IMPORTANT\n",
    "\n",
    "shared_df.unlink()  # IMPORTANT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the computations take much longer than copying the result, the advantage becomes clearer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A producer-consumer example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now consider the common scenario where we have several producer processes and one or more consumer processes. The idea here is that the producer processes do some work and create data, while consumer processes take this data and use it in some way. Consider for example the scenario described in a comment below, where we want to detect objects captured by several cameras, matching the same object among different cameras.\n",
    "\n",
    "To start, let's create two functions to produce and consume things. In our example, the producer will capture a frame from a camera, run some object detection algorithm and extract [object descriptors](https://docs.opencv.org/3.4/df/d54/tutorial_py_features_meaning.html) for that frame. To keep things simple, I will simulate these descriptors with large random matrices, introducing some random artificial delay to simulate processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "\n",
    "\n",
    "def camera_capture():\n",
    "    time.sleep(random.random())\n",
    "    \n",
    "    # 5000 descriptors with 250 features each\n",
    "    return np.random.random(size=(5000, 250))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The consumer process will take object descriptors from all cameras and try to match them. To keep things simple, let's assume that matching can be done via the [cosine similarity](https://en.wikipedia.org/wiki/Cosine_similarity), then we just need to compare the descriptors of all pairs of cameras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def object_matcher(frame, camera_objects):\n",
    "    # cameras is a list of SharedNumpyArray's\n",
    "    \n",
    "    for i in range(len(camera_objects) - 1):\n",
    "        if camera_objects[i] is None:\n",
    "            continue\n",
    "            \n",
    "        for j in range(i + 1, len(camera_objects)):\n",
    "            if camera_objects[j] is None:\n",
    "                continue\n",
    "\n",
    "            d1, d2 = camera_objects[i].read(), camera_objects[j].read()\n",
    "            similarity = (d1 @ d2.T) / d1.shape[1]\n",
    "            matches = np.sum(similarity > 0.4)\n",
    "            if matches > 0:\n",
    "                print('found', matches, 'matches at frame', frame, 'between cameras', i, 'and', j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these, we can now write the multiprocessing code. Generally, producer-consumer architectures make use of one or more [queues](https://docs.python.org/3/library/multiprocessing.html#pipes-and-queues) to share objects between processes. It is wise to limit the maximum size of the queue to a reasonable number of objects, so that in case the consumers are too slow compared to producers the computer's memory will not be filled up by objects waiting to be consumed.\n",
    "\n",
    "The producer process will simply use `camera_capture` to \"detect objects\", wrap the descriptors into a `SharedNumpyArray` and put that into a queue. Most importantly, here we `close` the object descriptors after putting them into the queue as we will not use them anymore in this process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from queue import Full\n",
    "\n",
    "\n",
    "# producer\n",
    "def camera_process(camera_idx, queue):\n",
    "    skips = 0\n",
    "    \n",
    "    # simulate 72 frames\n",
    "    for i in range(72):\n",
    "        objects = SharedNumpyArray(camera_capture())\n",
    "        try:\n",
    "            # timeout: how long to wait for a free spot in the queue\n",
    "            queue.put((camera_idx, i, objects), block=True, timeout=2)\n",
    "        except Full:\n",
    "            # queue is still full after the timeout, skip frame and move on.\n",
    "            # Note: these descriptors are now \"lost\" and won't be matched\n",
    "            # print('full queue! camera', camera_idx, 'skipping frame', i)\n",
    "            skips += 1\n",
    "        finally:\n",
    "            # *important:* always close the object as it is not needed anymore\n",
    "            objects.close()\n",
    "    \n",
    "    print('camera', camera_idx, 'done capturing, skipped', skips, 'frames')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The consumer process will receive object descriptors from the queue and run the matcher algorithm above. We use a list to store the latest descriptors of each camera and free up memory of older descriptors as new ones come in. Note that we do that by using `close` followed by `unlink`. Also note that we consider a period of three seconds without receiving objects as a signal that producers are done capturing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from queue import Empty\n",
    "\n",
    "# consumer\n",
    "def matching_process(num_cams, queue):\n",
    "    try:\n",
    "        # here we store the latest descriptors\n",
    "        camera_objects = [None] * num_cams\n",
    "        \n",
    "        while True:\n",
    "            camera, frame, objects = queue.get(True, timeout=3)\n",
    "            \n",
    "            # free memory of old descriptors\n",
    "            if camera_objects[camera] is not None:\n",
    "                camera_objects[camera].close()\n",
    "                camera_objects[camera].unlink()\n",
    "                \n",
    "            # store latest descriptors\n",
    "            camera_objects[camera] = objects\n",
    "            \n",
    "            # match every 8 frames because\n",
    "            # exhaustive matching every frame is too slow\n",
    "            if frame % 8 == 0:\n",
    "                object_matcher(frame, camera_objects)\n",
    "                \n",
    "    except Empty:\n",
    "        # empty queue, matching process quitting\n",
    "        pass\n",
    "\n",
    "    print('matcher quitting')\n",
    "    \n",
    "    # release all memory\n",
    "    for c in camera_objects:\n",
    "        if c is not None:\n",
    "            c.close()\n",
    "            c.unlink()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can finally weave these processes together and start the game!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "started matcher 1\n",
      "started matcher 2\n",
      "started camera 1\n",
      "started camera 2\n",
      "started camera 3\n",
      "started camera 4\n",
      "started camera 5\n",
      "capturing and matching in progress ...\n",
      "found 2 matches at frame 0 between cameras 3 and 4\n",
      "found 2 matches at frame 0 between cameras 2 and 3\n",
      "found 1 matches at frame 0 between cameras 0 and 3\n",
      "found 2 matches at frame 0 between cameras 2 and 4\n",
      "found 2 matches at frame 0 between cameras 3 and 4\n",
      "found 5 matches at frame 8 between cameras 0 and 2\n",
      "found 2 matches at frame 8 between cameras 2 and 3\n",
      "found 5 matches at frame 8 between cameras 0 and 4\n",
      "found 3 matches at frame 16 between cameras 1 and 3\n",
      "found 3 matches at frame 24 between cameras 3 and 4\n",
      "found 3 matches at frame 56 between cameras 0 and 3\n",
      "found 2 matches at frame 40 between cameras 1 and 2\n",
      "camera 2 done capturing, skipped 5 frames\n",
      "found 2 matches at frame 48 between cameras 1 and 2\n",
      "camera 1 done capturing, skipped 6 frames\n",
      "found 2 matches at frame 64 between cameras 0 and 3\n",
      "camera 3 done capturing, skipped 7 frames\n",
      "camera 0 done capturing, skipped 5 frames\n",
      "camera 4 done capturing, skipped 5 frames\n",
      "found 1 matches at frame 56 between cameras 1 and 2\n",
      "found 2 matches at frame 64 between cameras 2 and 3\n",
      "matcher quitting\n",
      "matcher quitting\n",
      "capturing completed\n",
      "total time taken: 77.68669128417969\n"
     ]
    }
   ],
   "source": [
    "num_cameras = 5\n",
    "num_matchers = 2\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# create queue\n",
    "queue = mp.Queue(maxsize=num_cameras * 10)\n",
    "\n",
    "# start consumer processes\n",
    "matchers = []\n",
    "for i in range(num_matchers):\n",
    "    m = mp.Process(target=matching_process, args=(num_cameras, queue))\n",
    "    m.start()\n",
    "    matchers.append(m)\n",
    "    print('started matcher', i + 1)\n",
    "\n",
    "# start producer processes\n",
    "cams = []\n",
    "for i in range(num_cameras):\n",
    "    c = mp.Process(target=camera_process, args=(i, queue))\n",
    "    c.start()\n",
    "    cams.append(c)\n",
    "    print('started camera', i + 1)\n",
    "\n",
    "print('capturing and matching in progress ...')\n",
    "\n",
    "# wait for all processes to quit\n",
    "for c in cams:\n",
    "    c.join()\n",
    "for m in matchers:\n",
    "    m.join()\n",
    "queue.close()\n",
    "\n",
    "print('capturing completed')\n",
    "\n",
    "end_time = time.time()\n",
    "print('total time taken:', end_time - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I hope this skeleton will be helpful in your application!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When is this solution (not) applicable?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As discussed above, there is still some copying involved, therefore it is not straightforward to tell when this solution might be faster. When the size of the result is large, but the computations required to obtain it are not so heavy, a sequential approach might be faster, but it is not clear where to draw the line.\n",
    "\n",
    "Another case to watch out is when you have large inputs, but small outputs. This solution is not necessary when you only read the input, but do not modify it. This is because the inputs follow a mechanism called [copy-on-write](https://en.wikipedia.org/wiki/Copy-on-write), i.e. are not copied _unless_ they are modified. This can be shown by slightly modifying the example above to return the sum of the convolution, instead of the convolution itself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 24/24 [00:17<00:00,  1.35it/s]\n"
     ]
    }
   ],
   "source": [
    "def do_work(args):\n",
    "    df, idx = args\n",
    "    kernel_idx = np.random.choice(df.shape[1], 20 * 20)\n",
    "    kernel = df.loc[idx][kernel_idx].values.reshape((20, 20))\n",
    "    result = convolve(df.values, kernel)\n",
    "    return result.sum()\n",
    "\n",
    "\n",
    "with mp.Pool() as pool:\n",
    "    tasks = ((df, df.index[idx]) for idx in process_rows)\n",
    "    result = pool.imap(do_work, tasks)\n",
    "    for res in tqdm(result, total=len(process_rows)):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is now as fast as the code above returning the whole result of the convolution.\n",
    "\n",
    "However, if you have classes the trick above becomes necessary again. I honestly do not know why. This also happens if the worker function is defined outside of the class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 24/24 [00:00<00:00, 181049.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not shared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 24/24 [00:41<00:00,  1.74s/it]\n"
     ]
    }
   ],
   "source": [
    "class Worker:\n",
    "    def __init__(self):\n",
    "        self.data = np.random.random(size=(10000, 10000))\n",
    "\n",
    "    @staticmethod\n",
    "    def work_not_shared(args):\n",
    "        data, i = args\n",
    "        return data[i].mean()\n",
    "    \n",
    "    def run_not_shared(self):\n",
    "        with mp.Pool() as pool:\n",
    "            tasks = [[self.data, idx] for idx in range(24)]\n",
    "            result = pool.imap(self.work_not_shared, tasks)\n",
    "            for res in tqdm(result, total=len(tasks)):\n",
    "                pass\n",
    "    \n",
    "    @staticmethod\n",
    "    def work_shared(args):\n",
    "        data, i = args\n",
    "        return data.read()[i].mean()\n",
    "    \n",
    "    def run_shared(self):\n",
    "        shared = SharedNumpyArray(self.data)\n",
    "        with mp.Pool() as pool:\n",
    "            tasks = [[shared, idx] for idx in range(24)]\n",
    "            result = pool.imap(self.work_shared, tasks)\n",
    "            for res in tqdm(result, total=len(tasks)):\n",
    "                pass\n",
    "        shared.unlink()\n",
    "\n",
    "\n",
    "print('Shared')\n",
    "Worker().run_shared()\n",
    "\n",
    "print('Not shared')\n",
    "Worker().run_not_shared()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Happy multiprocessing!\n",
    "\n",
    "This blog post, by the way, is fully contained in a jupyter notebook downloadable from [here](/attachments/multiprocessing-large-objects.ipynb)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
